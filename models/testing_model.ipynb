{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de603f0b-ca67-47d7-999e-f3a7963f3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_masked_prediction_notebook.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Add path to your modules if needed\n",
    "sys.path.append('/mnt/user-data/uploads')\n",
    "from jaisp_dataset_v4 import JAISPDatasetV4, ALL_BANDS\n",
    "from jaisp_foundation_v4 import JAISPFoundationV4\n",
    "\n",
    "def mask_region(img, mask_size=64, mask_value=0.0):\n",
    "    \"\"\"Mask out a square region in the center.\"\"\"\n",
    "    img_masked = img.clone()\n",
    "    h, w = img.shape[-2:]\n",
    "    y_start = (h - mask_size) // 2\n",
    "    x_start = (w - mask_size) // 2\n",
    "    img_masked[..., y_start:y_start+mask_size, x_start:x_start+mask_size] = mask_value\n",
    "    return img_masked, (y_start, x_start, mask_size)\n",
    "\n",
    "def predict_from_cross_view(model, view1_img, view1_rms, view1_band, \n",
    "                            view2_img, view2_rms, view2_band, device):\n",
    "    \"\"\"\n",
    "    Use view2 (e.g., Euclid) to predict what view1 (e.g., Rubin) should look like.\n",
    "    Returns the token embeddings from both views.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension if needed\n",
    "        if view1_img.dim() == 3:\n",
    "            view1_img = view1_img.unsqueeze(0)\n",
    "            view1_rms = view1_rms.unsqueeze(0)\n",
    "        if view2_img.dim() == 3:\n",
    "            view2_img = view2_img.unsqueeze(0)\n",
    "            view2_rms = view2_rms.unsqueeze(0)\n",
    "            \n",
    "        result1 = model.forward_student(\n",
    "            [view1_img.to(device)], \n",
    "            [view1_rms.to(device)], \n",
    "            [view1_band]\n",
    "        )\n",
    "        result2 = model.forward_student(\n",
    "            [view2_img.to(device)], \n",
    "            [view2_rms.to(device)], \n",
    "            [view2_band]\n",
    "        )\n",
    "    \n",
    "    return result1['z'], result2['z'], result1['grid_size'], result2['grid_size']\n",
    "\n",
    "def test_masked_prediction(\n",
    "    model_path: str,\n",
    "    rubin_dir: str,\n",
    "    euclid_dir: str,\n",
    "    n_samples: int = 5,\n",
    "    mask_size: int = 64,\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Create model with correct parameters\n",
    "    model = JAISPFoundationV4(\n",
    "        band_names=ALL_BANDS,\n",
    "        stem_ch=64,\n",
    "        embed_dim=256,\n",
    "        proj_dim=256,\n",
    "        depth=6,\n",
    "        patch_size=16,\n",
    "        shift_px=2,\n",
    "        shift_temp=0.07,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"‚úì Model loss: {checkpoint.get('loss', 'unknown'):.4f}\\n\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = JAISPDatasetV4(\n",
    "        rubin_dir=rubin_dir,\n",
    "        euclid_dir=euclid_dir,\n",
    "        augment=False,\n",
    "        cross_prob=1.0,  # Force cross-instrument pairs\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing masked prediction on {n_samples} samples...\\n\")\n",
    "    for i in range(min(n_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        # Get a Rubin-Euclid pair\n",
    "        if not sample['is_cross_pair']:\n",
    "            print(f\"Sample {i}: skipping (not cross-pair)\")\n",
    "            continue\n",
    "            \n",
    "        view1_img = sample['view1_image']\n",
    "        view1_rms = sample['view1_rms']\n",
    "        view1_band = sample['view1_band']\n",
    "        \n",
    "        view2_img = sample['view2_image']\n",
    "        view2_rms = sample['view2_rms']\n",
    "        view2_band = sample['view2_band']\n",
    "        \n",
    "        print(f\"Sample {i}: {view1_band} ({view1_img.shape[1:]}) ‚Üî {view2_band} ({view2_img.shape[1:]})\")\n",
    "        \n",
    "        # Mask view1\n",
    "        view1_masked, (y, x, size) = mask_region(view1_img, mask_size)\n",
    "        \n",
    "        # Get embeddings - full view1\n",
    "        z1_full, z2, gs1_full, gs2 = predict_from_cross_view(\n",
    "            model, view1_img, view1_rms, view1_band,\n",
    "            view2_img, view2_rms, view2_band, device\n",
    "        )\n",
    "        \n",
    "        # Get embeddings - masked view1\n",
    "        z1_masked, _, gs1_masked, _ = predict_from_cross_view(\n",
    "            model, view1_masked, view1_rms, view1_band,\n",
    "            view2_img, view2_rms, view2_band, device\n",
    "        )\n",
    "        \n",
    "        # Extract masked region tokens\n",
    "        H1, W1 = view1_img.shape[-2:]\n",
    "        token_grid_h, token_grid_w = gs1_full\n",
    "        \n",
    "        token_y_start = int(y / H1 * token_grid_h)\n",
    "        token_x_start = int(x / W1 * token_grid_w)\n",
    "        token_y_end = int((y + size) / H1 * token_grid_h)\n",
    "        token_x_end = int((x + size) / W1 * token_grid_w)\n",
    "        \n",
    "        # Reshape to spatial grid\n",
    "        B, N, D = z1_full.shape\n",
    "        z1_full_grid = z1_full.reshape(B, token_grid_h, token_grid_w, D)\n",
    "        z1_masked_grid = z1_masked.reshape(B, token_grid_h, token_grid_w, D)\n",
    "        z2_grid = z2.reshape(B, gs2[0], gs2[1], D)\n",
    "        \n",
    "        # Extract masked region\n",
    "        z1_full_region = z1_full_grid[0, token_y_start:token_y_end, token_x_start:token_x_end]\n",
    "        z1_masked_region = z1_masked_grid[0, token_y_start:token_y_end, token_x_start:token_x_end]\n",
    "        \n",
    "        # For z2, map to corresponding region\n",
    "        token2_y_start = int(token_y_start / token_grid_h * gs2[0])\n",
    "        token2_x_start = int(token_x_start / token_grid_w * gs2[1])\n",
    "        token2_y_end = int(token_y_end / token_grid_h * gs2[0])\n",
    "        token2_x_end = int(token_x_end / token_grid_w * gs2[1])\n",
    "        z2_region = z2_grid[0, token2_y_start:token2_y_end, token2_x_start:token2_x_end]\n",
    "        \n",
    "        # Flatten and compute similarities\n",
    "        z1_full_flat = z1_full_region.flatten()\n",
    "        z1_masked_flat = z1_masked_region.flatten()\n",
    "        z2_flat = z2_region.flatten()\n",
    "        \n",
    "        # Cosine similarity\n",
    "        cos_full_vs_masked = torch.cosine_similarity(\n",
    "            z1_full_flat.unsqueeze(0), \n",
    "            z1_masked_flat.unsqueeze(0), \n",
    "            dim=1\n",
    "        ).item()\n",
    "        \n",
    "        # For cross-view, handle size mismatch\n",
    "        if z1_full_flat.shape[0] != z2_flat.shape[0]:\n",
    "            cos_full_vs_cross = torch.corrcoef(torch.stack([z1_full_flat, z2_flat]))[0,1].item()\n",
    "        else:\n",
    "            cos_full_vs_cross = torch.cosine_similarity(\n",
    "                z1_full_flat.unsqueeze(0), \n",
    "                z2_flat.unsqueeze(0), \n",
    "                dim=1\n",
    "            ).item()\n",
    "        \n",
    "        results.append({\n",
    "            'sample_idx': i,\n",
    "            'view1_band': view1_band,\n",
    "            'view2_band': view2_band,\n",
    "            'cos_full_vs_masked': cos_full_vs_masked,\n",
    "            'cos_full_vs_cross': cos_full_vs_cross,\n",
    "            'images': {\n",
    "                'view1_original': view1_img.squeeze().cpu().numpy(),\n",
    "                'view1_masked': view1_masked.squeeze().cpu().numpy(),\n",
    "                'view2': view2_img.squeeze().cpu().numpy(),\n",
    "            },\n",
    "            'mask_region': (y, x, size),\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚Üí Full vs Masked similarity: {cos_full_vs_masked:.3f}\")\n",
    "        print(f\"  ‚Üí Full vs Cross-view similarity: {cos_full_vs_cross:.3f}\\n\")\n",
    "    \n",
    "    # Visualize\n",
    "    if len(results) == 0:\n",
    "        print(\"‚ùå No cross-pairs found!\")\n",
    "        return results\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(len(results), 3, figsize=(14, 4*len(results)))\n",
    "    if len(results) == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    \n",
    "    for idx, result in enumerate(results):\n",
    "        y, x, size = result['mask_region']\n",
    "        \n",
    "        # Original\n",
    "        img_orig = result['images']['view1_original']\n",
    "        v1, v99 = np.percentile(img_orig, [1, 99])\n",
    "        axes[idx, 0].imshow(img_orig, cmap='gray', vmin=v1, vmax=v99)\n",
    "        rect = plt.Rectangle((x, y), size, size, fill=False, edgecolor='red', linewidth=2)\n",
    "        axes[idx, 0].add_patch(rect)\n",
    "        axes[idx, 0].set_title(f\"{result['view1_band']} (original)\\nfull vs masked: {result['cos_full_vs_masked']:.3f}\", fontsize=10)\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        # Masked\n",
    "        img_masked = result['images']['view1_masked']\n",
    "        axes[idx, 1].imshow(img_masked, cmap='gray', vmin=v1, vmax=v99)\n",
    "        axes[idx, 1].set_title(f\"{result['view1_band']} (masked 64√ó64)\", fontsize=10)\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        # Cross-view\n",
    "        img_cross = result['images']['view2']\n",
    "        v1c, v99c = np.percentile(img_cross, [1, 99])\n",
    "        axes[idx, 2].imshow(img_cross, cmap='gray', vmin=v1c, vmax=v99c)\n",
    "        axes[idx, 2].set_title(f\"{result['view2_band']} (reference)\\nfull vs cross: {result['cos_full_vs_cross']:.3f}\", fontsize=10)\n",
    "        axes[idx, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    avg_full_masked = np.mean([r['cos_full_vs_masked'] for r in results])\n",
    "    avg_full_cross = np.mean([r['cos_full_vs_cross'] for r in results])\n",
    "    \n",
    "    print(f\"\\nAverage similarity (full vs masked):     {avg_full_masked:.3f}\")\n",
    "    print(f\"Average similarity (full vs cross-view):  {avg_full_cross:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Interpretation:\")\n",
    "    if avg_full_masked > 0.9:\n",
    "        print(f\"  ‚ö†Ô∏è  Full/Masked similarity is HIGH ({avg_full_masked:.3f}) - masking barely changed representation\")\n",
    "    elif avg_full_masked < 0.7:\n",
    "        print(f\"  ‚úì  Full/Masked similarity is LOW ({avg_full_masked:.3f}) - masked region matters to model\")\n",
    "    else:\n",
    "        print(f\"  ~  Full/Masked similarity is MODERATE ({avg_full_masked:.3f})\")\n",
    "    \n",
    "    if avg_full_cross > 0.8:\n",
    "        print(f\"  ‚úì  Full/Cross similarity is HIGH ({avg_full_cross:.3f}) - cross-view captures masked region info\")\n",
    "    elif avg_full_cross < 0.5:\n",
    "        print(f\"  ‚ö†Ô∏è  Full/Cross similarity is LOW ({avg_full_cross:.3f}) - cross-view doesn't help predict\")\n",
    "    else:\n",
    "        print(f\"  ~  Full/Cross similarity is MODERATE ({avg_full_cross:.3f})\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fa7976-4c8d-4199-a4f1-2fd11e524d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/jaisp_v4/best.pt\n",
      "‚úì Loaded model from epoch 18\n",
      "‚úì Model loss: 0.0157\n",
      "\n",
      "JAISPDatasetV4: 144 tiles (native resolutions)\n",
      "  pairing: cross_prob=1.0, balance_usage=True, precompute_available=True\n",
      "Testing masked prediction on 5 samples...\n",
      "\n",
      "Sample 0: rubin_z (torch.Size([512, 512])) ‚Üî euclid_J (torch.Size([1050, 1050]))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JAISPFoundationV4' object has no attribute 'forward_student'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_masked_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints/jaisp_v4/best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrubin_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/rubin_tiles_ecdfs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meuclid_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/euclid_tiles_ecdfs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m, in \u001b[0;36mtest_masked_prediction\u001b[0;34m(model_path, rubin_dir, euclid_dir, n_samples, mask_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m view1_masked, (y, x, size) \u001b[38;5;241m=\u001b[39m mask_region(view1_img, mask_size)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Get embeddings - full view1\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m z1_full, z2, gs1_full, gs2 \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_from_cross_view\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview1_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview1_rms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview1_band\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview2_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview2_rms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview2_band\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Get embeddings - masked view1\u001b[39;00m\n\u001b[1;32m    121\u001b[0m z1_masked, _, gs1_masked, _ \u001b[38;5;241m=\u001b[39m predict_from_cross_view(\n\u001b[1;32m    122\u001b[0m     model, view1_masked, view1_rms, view1_band,\n\u001b[1;32m    123\u001b[0m     view2_img, view2_rms, view2_band, device\n\u001b[1;32m    124\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mpredict_from_cross_view\u001b[0;34m(model, view1_img, view1_rms, view1_band, view2_img, view2_rms, view2_band, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m         view2_img \u001b[38;5;241m=\u001b[39m view2_img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m         view2_rms \u001b[38;5;241m=\u001b[39m view2_rms\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     result1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_student\u001b[49m(\n\u001b[1;32m     38\u001b[0m         [view1_img\u001b[38;5;241m.\u001b[39mto(device)], \n\u001b[1;32m     39\u001b[0m         [view1_rms\u001b[38;5;241m.\u001b[39mto(device)], \n\u001b[1;32m     40\u001b[0m         [view1_band]\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     result2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_student(\n\u001b[1;32m     43\u001b[0m         [view2_img\u001b[38;5;241m.\u001b[39mto(device)], \n\u001b[1;32m     44\u001b[0m         [view2_rms\u001b[38;5;241m.\u001b[39mto(device)], \n\u001b[1;32m     45\u001b[0m         [view2_band]\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], result2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], result1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_size\u001b[39m\u001b[38;5;124m'\u001b[39m], result2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'JAISPFoundationV4' object has no attribute 'forward_student'"
     ]
    }
   ],
   "source": [
    "results = test_masked_prediction(\n",
    "    model_path='checkpoints/jaisp_v4/best.pt',\n",
    "    rubin_dir='../data/rubin_tiles_ecdfs/',\n",
    "    euclid_dir='../data/euclid_tiles_ecdfs/',\n",
    "    n_samples=5,\n",
    "    mask_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e6eb8-6b7f-47bc-bf51-0620d8599933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
