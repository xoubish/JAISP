{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40db64d2-ae43-4cb9-b9c3-577baffd5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AstroStem(nn.Module):\n",
    "    \"\"\"\n",
    "    Instrument-specific 'Stems' that project Science+RMS into a \n",
    "    common latent resolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, input_size, embed_dim=256):\n",
    "        super().__init__()\n",
    "        # Rubin: 512 -> stride 16 results in 32x32 tokens\n",
    "        # Euclid: 1050 -> center crop to 1024 -> stride 32 results in 32x32 tokens\n",
    "        self.input_size = input_size\n",
    "        stride = 16 if input_size == 512 else 32\n",
    "        \n",
    "        # We concatenate Science + RMS, doubling the input channels\n",
    "        self.proj = nn.Conv2d(in_channels * 2, embed_dim, kernel_size=stride, stride=stride)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 32*32, embed_dim))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, img, rms):\n",
    "        # 1. Uncertainty Integration: Concatenate flux and noise\n",
    "        x = torch.cat([img, rms], dim=1) # (B, 2*C, H, W)\n",
    "\n",
    "        # 2. Resolution Matching\n",
    "        if self.input_size == 1050:\n",
    "            # Center crop 1050 to 1024 to maintain power-of-2 patching\n",
    "            x = x[:, :, 13:-13, 13:-13]\n",
    "            \n",
    "        # 3. Tokenization\n",
    "        tokens = self.proj(x).flatten(2).transpose(1, 2) # (B, 1024, embed_dim)\n",
    "        tokens = tokens + self.pos_embed\n",
    "        return self.norm(tokens)\n",
    "\n",
    "class Stage1Foundation(nn.Module):\n",
    "    \"\"\"\n",
    "    Roadmap Stage 1: Cross-survey representation.\n",
    "    Learns consistent features across Rubin (ground) and Euclid (space).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=256, depth=8, num_heads=8):\n",
    "        super().__init__()\n",
    "        # Multi-scale stems\n",
    "        self.rubin_stem = AstroStem(in_channels=6, input_size=512, embed_dim=embed_dim)\n",
    "        self.euclid_stem = AstroStem(in_channels=4, input_size=1050, embed_dim=embed_dim)\n",
    "\n",
    "        # Shared trunk: Windowed Attention Transformer\n",
    "        # This allows Rubin and Euclid to be processed by the same physical weights\n",
    "        self.trunk = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=embed_dim*4,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        # JEPA Predictor Head\n",
    "        # Attempts to predict Euclid latents from Rubin latents (cross-instrument bridge)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, r_img, r_rms, e_img, e_rms):\n",
    "        # Map both instruments to a shared 32x32 latent grid\n",
    "        r_latents_init = self.rubin_stem(r_img, r_rms)\n",
    "        e_latents_init = self.euclid_stem(e_img, e_rms)\n",
    "        \n",
    "        # Process through shared trunk\n",
    "        # r_latents represents the Rubin-based view of the sky\n",
    "        # e_latents represents the Euclid-based view of the sky\n",
    "        r_latents = self.trunk(r_latents_init)\n",
    "        e_latents = self.trunk(e_latents_init)\n",
    "        \n",
    "        # JEPA Prediction: Can we see the space-based truth from the ground?\n",
    "        e_pred = self.predictor(r_latents)\n",
    "        \n",
    "        return {\n",
    "            \"r_latents\": r_latents, \n",
    "            \"e_latents\": e_latents, \n",
    "            \"e_pred\": e_pred\n",
    "        }\n",
    "\n",
    "def jepa_loss_fn(outputs, e_rms_map, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Latent-space loss weighted by the target's S/N.\n",
    "    We detatch the target latents (e_latents) following the JEPA/DINO recipe.\n",
    "    \"\"\"\n",
    "    e_target = outputs[\"e_latents\"].detach()\n",
    "    e_pred = outputs[\"e_pred\"]\n",
    "    \n",
    "    # Calculate MSE\n",
    "    loss = F.mse_loss(e_pred, e_target, reduction='none') # (B, 1024, embed_dim)\n",
    "    \n",
    "    # Optional: You can use the e_rms_map to mask out latent patches \n",
    "    # that correspond to saturated or missing data in the target Euclid tile.\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b96c18-ebe1-4894-90de-0e6750de39d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
